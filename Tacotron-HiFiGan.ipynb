{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28257,"status":"ok","timestamp":1747721103406,"user":{"displayName":"Thiệt Trần Đình Ngô","userId":"04054980513334747873"},"user_tz":-420},"id":"K5yvOeKhCZMo","outputId":"2dbb1da5-f70a-41d6-933c-ef5525561659"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3111,"status":"ok","timestamp":1747721120726,"user":{"displayName":"Thiệt Trần Đình Ngô","userId":"04054980513334747873"},"user_tz":-420},"id":"v6b3fKRGCgKl","outputId":"2e714d5a-4092-4933-9dd4-2a59322e397e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting hickle\n","  Downloading hickle-5.0.3-py3-none-any.whl.metadata (22 kB)\n","Requirement already satisfied: h5py>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from hickle) (3.13.0)\n","Requirement already satisfied: numpy!=1.20,>=1.8 in /usr/local/lib/python3.11/dist-packages (from hickle) (2.0.2)\n","Downloading hickle-5.0.3-py3-none-any.whl (107 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/108.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: hickle\n","Successfully installed hickle-5.0.3\n"]}],"source":["pip install hickle"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":13859,"status":"ok","timestamp":1747721140730,"user":{"displayName":"Thiệt Trần Đình Ngô","userId":"04054980513334747873"},"user_tz":-420},"id":"hUx4IGKbChrO"},"outputs":[],"source":["from string import printable\n","import numpy as np\n","import pandas as pd\n","from csv import QUOTE_NONE\n","import hickle as hkl\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torchaudio\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision.transforms import Compose, ToTensor"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":44,"status":"ok","timestamp":1747721147019,"user":{"displayName":"Thiệt Trần Đình Ngô","userId":"04054980513334747873"},"user_tz":-420},"id":"JUHJAk8qCjRS"},"outputs":[],"source":["SEED = 42\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)"]},{"cell_type":"markdown","metadata":{"id":"Emnq1Z_9CrdI"},"source":["**Chuẩn bị dữ liệu**"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":1652,"status":"ok","timestamp":1747721151050,"user":{"displayName":"Thiệt Trần Đình Ngô","userId":"04054980513334747873"},"user_tz":-420},"id":"vkqptRMnCpTQ"},"outputs":[],"source":["ds_metadata = pd.read_csv(\"/content/drive/MyDrive/project1/LJSpeech-1.1/LJSpeech/metadata.csv\", sep = \"|\", quoting = QUOTE_NONE, names = [\"WAV Name\", \"Transcripts\", \"Normalized Transcripts\"])"]},{"cell_type":"code","source":["ds_metadata.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"myQbwoxpS67I","executionInfo":{"status":"ok","timestamp":1747721152319,"user_tz":-420,"elapsed":70,"user":{"displayName":"Thiệt Trần Đình Ngô","userId":"04054980513334747873"}},"outputId":"17ea2f45-5a19-410a-d437-37932b81a471"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     WAV Name                                        Transcripts  \\\n","0  LJ001-0001  Printing, in the only sense with which we are ...   \n","1  LJ001-0002                     in being comparatively modern.   \n","2  LJ001-0003  For although the Chinese took impressions from...   \n","3  LJ001-0004  produced the block books, which were the immed...   \n","4  LJ001-0005  the invention of movable metal letters in the ...   \n","\n","                              Normalized Transcripts  \n","0  Printing, in the only sense with which we are ...  \n","1                     in being comparatively modern.  \n","2  For although the Chinese took impressions from...  \n","3  produced the block books, which were the immed...  \n","4  the invention of movable metal letters in the ...  "],"text/html":["\n","  <div id=\"df-cebdd892-2100-4fc2-b708-bcc483fbcc47\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>WAV Name</th>\n","      <th>Transcripts</th>\n","      <th>Normalized Transcripts</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>LJ001-0001</td>\n","      <td>Printing, in the only sense with which we are ...</td>\n","      <td>Printing, in the only sense with which we are ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>LJ001-0002</td>\n","      <td>in being comparatively modern.</td>\n","      <td>in being comparatively modern.</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>LJ001-0003</td>\n","      <td>For although the Chinese took impressions from...</td>\n","      <td>For although the Chinese took impressions from...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>LJ001-0004</td>\n","      <td>produced the block books, which were the immed...</td>\n","      <td>produced the block books, which were the immed...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>LJ001-0005</td>\n","      <td>the invention of movable metal letters in the ...</td>\n","      <td>the invention of movable metal letters in the ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cebdd892-2100-4fc2-b708-bcc483fbcc47')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-cebdd892-2100-4fc2-b708-bcc483fbcc47 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-cebdd892-2100-4fc2-b708-bcc483fbcc47');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-f3c6a5de-01b4-4608-9f6f-fde5aa500ff0\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f3c6a5de-01b4-4608-9f6f-fde5aa500ff0')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-f3c6a5de-01b4-4608-9f6f-fde5aa500ff0 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"ds_metadata","summary":"{\n  \"name\": \"ds_metadata\",\n  \"rows\": 13100,\n  \"fields\": [\n    {\n      \"column\": \"WAV Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 13100,\n        \"samples\": [\n          \"LJ015-0247\",\n          \"LJ028-0475\",\n          \"LJ013-0049\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Transcripts\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 13074,\n        \"samples\": [\n          \"He acknowledged the encounter with the police officer on the second floor.\",\n          \"Oswald's behavior after the assassination throws little light on his motives.\",\n          \"The Commission also regards the security arrangements worked out by Lawson and Sorrels at Love Field as entirely adequate.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Normalized Transcripts\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 13074,\n        \"samples\": [\n          \"He acknowledged the encounter with the police officer on the second floor.\",\n          \"Oswald's behavior after the assassination throws little light on his motives.\",\n          \"The Commission also regards the security arrangements worked out by Lawson and Sorrels at Love Field as entirely adequate.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":6}]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1747721156452,"user":{"displayName":"Thiệt Trần Đình Ngô","userId":"04054980513334747873"},"user_tz":-420},"id":"0YA1uSlBCwad"},"outputs":[],"source":["ds_metadata.drop('Transcripts', axis = 1, inplace = True)"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":197,"status":"ok","timestamp":1747721158984,"user":{"displayName":"Thiệt Trần Đình Ngô","userId":"04054980513334747873"},"user_tz":-420},"id":"0Afclg7oCx_2"},"outputs":[],"source":["ds_metadata['Normalized Transcripts'] = ds_metadata['Normalized Transcripts'].str.lower()\n","\n","ds_metadata['Normalized Transcripts'] = ds_metadata['Normalized Transcripts'].str.replace(' +',' ', regex = True) \\\n","                                                                                 .replace('ü','u', regex = True)  \\\n","                                                                                 .replace('“','\"', regex = True)  \\\n","                                                                                 .replace('”', '\"', regex = True) \\\n","                                                                                 .replace('’', '\\'', regex = True) \\\n","                                                                                 .replace(\"i.e.\", \"i e \") \\\n","                                                                                 .replace(\";\", \"\") \\\n","                                                                                 .replace (\"  \", \" \")"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36,"status":"ok","timestamp":1747721160699,"user":{"displayName":"Thiệt Trần Đình Ngô","userId":"04054980513334747873"},"user_tz":-420},"id":"djAtyxa2C0TJ","outputId":"33cec165-c9ae-4d39-8107-da870a194063"},"outputs":[{"output_type":"stream","name":"stdout","text":["abcdefghijklmnopqrstuvwxyz!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \n","59\n"]}],"source":["#Từ\n","vocab = printable\n","for ch in ['\\t', '\\n', '\\r', '\\x0b', '\\x0c']:\n","    vocab = vocab.replace(ch, '')\n","\n","vocab = vocab[10:]\n","vocab = vocab.replace('ABCDEFGHIJKLMNOPQRSTUVWXYZ', '')\n","print(vocab)\n","print(len(vocab))"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":253,"status":"ok","timestamp":1747721162453,"user":{"displayName":"Thiệt Trần Đình Ngô","userId":"04054980513334747873"},"user_tz":-420},"id":"HZ6IBSM2C28K"},"outputs":[],"source":["# mã hóa từ sang int\n","\n","def ch_to_int(transcript):\n","    try:\n","        return [vocab.index(ch) for ch in transcript]\n","    except:\n","        return 'bad encoding'\n","\n","ds_metadata['One-Hot Encoding'] = ds_metadata['Normalized Transcripts'].apply(ch_to_int)\n","ds_metadata = ds_metadata[ds_metadata['One-Hot Encoding'] != 'bad encoding']\n","ds_metadata = ds_metadata.reset_index()"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"Zh_0OZXADRcu","executionInfo":{"status":"error","timestamp":1747721616337,"user_tz":-420,"elapsed":94568,"user":{"displayName":"Thiệt Trần Đình Ngô","userId":"04054980513334747873"}},"outputId":"0349c2f9-e162-4ff6-86fa-84d430d1b5de"},"outputs":[{"output_type":"stream","name":"stdout","text":["Đang sử dụng: cuda\n"]},{"output_type":"stream","name":"stderr","text":["  2%|▏         | 222/13100 [01:34<1:31:17,  2.35it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-e4611dff19ce>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# Gọi hàm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/project1/LJSpeech-1.1/LJSpeech/wavs/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mwav_to_melspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-13-e4611dff19ce>\u001b[0m in \u001b[0;36mwav_to_melspec\u001b[0;34m(root)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# Load và chuyển wave sang GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mwave\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".wav\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mwave\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwave\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchaudio/_backend/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \"\"\"\n\u001b[1;32m    204\u001b[0m         \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchaudio/_backend/ffmpeg.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mbuffer_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4096\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     ) -> Tuple[torch.Tensor, int]:\n\u001b[0;32m--> 297\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchaudio/_backend/ffmpeg.py\u001b[0m in \u001b[0;36mload_audio\u001b[0;34m(src, frame_offset, num_frames, convert, channels_first, format, buffer_size)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"vorbis\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ogg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0msample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_src_stream_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_audio_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mfilter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_load_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torio/io/_streaming_media_decoder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, format, option, buffer_size)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_be\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mffmpeg_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamingMediaDecoderFileObj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_be\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mffmpeg_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamingMediaDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_be\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_best_audio_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import os\n","from tqdm import tqdm\n","\n","# Kiểm tra GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Đang sử dụng:\", device)\n","\n","# Các thông số xử lý âm thanh\n","SAMPLE_RATE = 22050\n","N_FFT = 2048\n","FRAME_SIZE = 50\n","HOP_SIZE = 12.5\n","N_MELS = 80\n","\n","# Load metadata\n","ds_metadata = pd.read_csv('/content/drive/MyDrive/project1/LJSpeech-1.1/LJSpeech/metadata.csv', sep='|', header=None)\n","ds_metadata.columns = ['WAV Name', 'Transcripts', 'Normalized Transcripts']\n","ds_metadata['WAV Name'] = ds_metadata['WAV Name'].str.strip()\n","\n","def wav_to_melspec(root):\n","    data = []\n","\n","    # MelSpectrogram to GPU\n","    w2ms_trans = torchaudio.transforms.MelSpectrogram(SAMPLE_RATE,\n","                                                      n_fft=N_FFT,\n","                                                      win_length= FRAME_SIZE * SAMPLE_RATE // 1000,\n","                                                      hop_length=int(HOP_SIZE * SAMPLE_RATE // 1000),\n","                                                      f_min=0,\n","                                                      f_max=8000,\n","                                                      n_mels=N_MELS,\n","                                                      window_fn=torch.hann_window,\n","                                                      power=1.0,\n","                                                      center=False,\n","                                                      norm='slaney',\n","                                                      mel_scale='slaney'\n","                                                      ).to(device)\n","\n","    for i in tqdm(range(len(ds_metadata))):\n","        file_name = ds_metadata.loc[i, 'WAV Name']\n","        try:\n","            # Load và chuyển wave sang GPU\n","            wave, _ = torchaudio.load(os.path.join(root, file_name + \".wav\"))\n","            wave = wave.to(device)\n","\n","            # Chuyển sang MelSpec\n","            wav2melspec = w2ms_trans(wave)\n","            wav2melspec = torch.clip(wav2melspec, min=1e-5)\n","            amp2db = torch.log(wav2melspec)\n","\n","            # Stop token\n","            stop_token = np.zeros(amp2db.shape[-1])\n","            stop_token[-1] = 1.0\n","\n","            # Chuyển về CPU để lưu\n","            data.append((\n","                file_name,\n","                ds_metadata.loc[i, 'Normalized Transcripts'],\n","                amp2db.cpu().numpy()[0],\n","                stop_token\n","            ))\n","\n","        except Exception as e:\n","            print(f\"Lỗi {file_name}: {e}\")\n","\n","    hkl.dump(data, '/content/drive/MyDrive/project1/LJSpeech-1.1/LJSpeech/ljspeech_gpu.hkl', compression='gzip')\n","\n","# Gọi hàm\n","root = \"/content/drive/MyDrive/project1/LJSpeech-1.1/LJSpeech/wavs/\"\n","wav_to_melspec(root)\n"]},{"cell_type":"code","source":["!mv /content/ljspeech_gpu.hkl /content/drive/MyDrive/project1/LJSpeech-1.1/LJSpeech"],"metadata":{"id":"Ua1SHDBU2OKI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ngKiH9saDVsY"},"source":["**Tacotron2**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IkyJsWGZDUja"},"outputs":[],"source":["EPOCHS = 200\n","BATCH_SIZE = 64 # 16\n","VOCAB_SIZE = 59\n","EMBEDDING_SIZE = 512\n","NR_MELS = 80\n","LR = 0.001\n","EPS = 1e-08 #1e-06\n","WEIGHT_DECAY = 1e-06\n","THRESHOLD = 0.5\n","MAX_DEC_STEPS = 1000\n","CHKP_PATH = '/content/drive/MyDrive/project1/LJSpeech-1.1/LJSpeech/taco2_1000_ds_size.pth'\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"ebwMvpR6Dc__"},"source":["**Dataset**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pk9Kyqk-DaY8"},"outputs":[],"source":["#@title Dataset { form-width: \"22%\" }\n","class LJSpeech(Dataset):\n","    def __init__(self, ds_path):\n","        self.ds = hkl.load(ds_path)\n","        self.sz = 12000 + 95 + 500 # train, valid, test\n","        self.ds = self.ds[:self.sz]\n","\n","    def __len__(self):\n","        #return len(self.ds)\n","        return self.sz\n","\n","    def __getitem__(self, index):\n","        transcript = torch.LongTensor(self.ds[index][2])\n","        melspec = torch.FloatTensor(self.ds[index][3])\n","        stop_token = torch.FloatTensor(self.ds[index][4])\n","        return transcript, melspec.transpose(1, 0), stop_token # transponse melspec to pad it in collate\n","\n","    def min_max(self):\n","        min, max = 10000, -10000\n","        for (_, melspec, _, _, _) in train_loader:\n","            melspec_max = torch.max(melspec)\n","            if melspec_max > max:\n","                max = melspec_max\n","\n","            melspec_min = torch.min(melspec)\n","            if melspec_min < min:\n","                min = melspec_min\n","\n","        print(min, max)\n","\n","def collate_pad(batch):\n","    transcripts, melspecs, stop_tokens = zip(*batch)\n","    mel_lens = torch.LongTensor([mel.shape[0] for mel in melspecs])\n","    trans_lens = torch.LongTensor([t.shape[0] for t in transcripts])\n","\n","    transcripts = nn.utils.rnn.pad_sequence(transcripts, batch_first = True)\n","    melspecs = nn.utils.rnn.pad_sequence(melspecs, batch_first = True)\n","    stop_tokens = nn.utils.rnn.pad_sequence(stop_tokens, batch_first = True)\n","    for i, elem in enumerate(stop_tokens):\n","        elem[mel_lens[i] - 1:] = 1.0\n","    return transcripts, melspecs.permute(0, 2, 1), stop_tokens, mel_lens, trans_lens"]},{"cell_type":"markdown","metadata":{"id":"qeKb-H7KDmO8"},"source":["**Encoder**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DcS3pMWNDgvu"},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self, vocab_size, embedding_size):\n","        super().__init__()\n","\n","        self.embedding_size = embedding_size\n","\n","        self.embed = nn.Embedding(vocab_size, embedding_size)\n","        self.conv = nn.Sequential(\n","            nn.Conv1d(embedding_size, embedding_size, 5, padding = 2, bias = True),\n","            nn.BatchNorm1d(embedding_size),\n","            nn.ReLU(),\n","            nn.Dropout(),\n","            nn.Conv1d(embedding_size, embedding_size, 5, padding = 2, bias = True),\n","            nn.BatchNorm1d(embedding_size),\n","            nn.ReLU(),\n","            nn.Dropout(),\n","            nn.Conv1d(embedding_size, embedding_size, 5, padding = 2, bias = True),\n","            nn.BatchNorm1d(embedding_size),\n","            nn.ReLU(),\n","            nn.Dropout()\n","        )\n","\n","        for layer in self.conv:\n","            if isinstance(layer, nn.Conv1d):\n","                torch.nn.init.xavier_uniform_(layer.weight, gain = torch.nn.init.calculate_gain('relu'))\n","\n","        self.bi_lstm = nn.LSTM(embedding_size, embedding_size // 2,\n","                               batch_first = True, bidirectional = True)\n","        self.drop = nn.Dropout(0.1)\n","\n","    def forward(self, x):\n","        x = self.embed(x)\n","        x = x.transpose(1, 2)\n","        #assert x.shape[:2] == (BATCH_SIZE, self.embedding_size), x.shape\n","        x = self.conv(x)\n","        x = x.transpose(1, 2)\n","        #assert x.shape[-1] == self.embedding_size, x.shape\n","        x, _ = self.bi_lstm(x)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"10dg2RPlDsxQ"},"source":["**Aligner**\n","Căn chỉnh từ bằng cơ chế attention"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MCH-DziyDpnc"},"outputs":[],"source":["class AlignNN(nn.Module):\n","    def __init__(self, enc_hidden_size, dec_hidden_size):\n","        '''\n","        Calculates the alignment of the arguments, representing the importance of each encoder output.\n","        Args:\n","            enc_hidden_size: output size of encoder at step j\n","            dec_hidden_size: size of previous hidden state of the decoder\n","        '''\n","        super().__init__()\n","        self.ll_dec_prev_hidden = nn.Linear(dec_hidden_size, 128)\n","        self.ll_enc_hidden = nn.Linear(enc_hidden_size, 128)\n","        self.ll_prev_step_att = nn.Linear(32, 128)\n","        self.ll_out = nn.Linear(128, 1)\n","\n","        self.proj1 = None # saves encoder hidden LL output\n","\n","\n","    def forward(self, dec_prev_hidden, prev_step_att):\n","        proj2 = self.ll_dec_prev_hidden(dec_prev_hidden) # (BATCH_SIZE, seq_len = 1, hidden_size = 128)\n","        #assert len(proj2.shape) == 3\n","        #assert proj1.shape[2] == proj2.shape[2]\n","        proj3 = self.ll_prev_step_att(prev_step_att.transpose(2, 1)) # TODO: should it be transpose 1, 2?\n","        #assert len(proj3.shape) == 3\n","        #assert proj1.shape[2] == proj3.shape[2]\n","        return self.ll_out(torch.tanh(self.proj1 + proj2 + proj3)).squeeze(2)\n","\n","class Attention(nn.Module):\n","    def __init__(self, enc_hidden_size, dec_hidden_size):\n","        '''\n","        Calculates attention (https://arxiv.org/abs/1409.0473)\n","        Args:\n","            enc_hidden_size: output size of encoder at step j\n","            dec_hidden_size: size of previous hidden state of the decoder\n","        '''\n","        super().__init__()\n","        # vanilla, Tacotron 2 paper version (my version) doesn't concat previous\n","        # step attention with the cumulative attention, so only 1 channel is needed\n","        # self.location_att = nn.Conv1d(1, 32, 31, padding = 15)\n","\n","        self.location_att = nn.Conv1d(2, 32, 31, padding = 15)\n","        self.align_nn = AlignNN(enc_hidden_size, dec_hidden_size)\n","        self.mask = None\n","\n","\n","    def forward(self, enc_out, dec_prev_h, prev_step_att):\n","        # location features\n","        location_features = self.location_att(prev_step_att)\n","\n","        # calculate the attention weights\n","        att_weights = self.align_nn(dec_prev_h, location_features) # (BATCH_SIZE, seq_len)\n","\n","        if self.mask is not None:\n","            att_weights.data.masked_fill_(self.mask, -float('inf'))\n","\n","        #assert att_weights.shape == (BATCH_SIZE, enc_out.shape[1])\n","        att_weights = torch.softmax(att_weights, dim = 1)\n","        # return the current step context vector (BATCH_SIZE, EMBEDDING_SIZE = 512)\n","        return att_weights, torch.bmm(att_weights.unsqueeze(1), enc_out).squeeze(1)"]},{"cell_type":"markdown","metadata":{"id":"6tR4yQ_SEGTn"},"source":["**Decoder**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L2K_G0WMED9Y"},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, melspec_frame_shape):\n","        super().__init__()\n","        self.pre_net = nn.Sequential(\n","            nn.Linear(melspec_frame_shape, 256),\n","            nn.ReLU(),\n","            nn.Dropout(),\n","            nn.Linear(256, 256),\n","            nn.ReLU(),\n","            nn.Dropout()\n","        )\n","\n","        self.lstm = nn.LSTM(EMBEDDING_SIZE + 256, 1024, 2, batch_first = True, dropout = 0.1)\n","        self.drop = nn.Dropout(0.1)\n","        self.linear = nn.Linear(EMBEDDING_SIZE + 1024, 80)\n","        self.stop_linear = nn.Sequential(\n","            nn.Linear(EMBEDDING_SIZE + 1024, 1),\n","        )\n","        self.post_net = nn.Sequential(\n","            nn.Conv1d(NR_MELS, 512, 5, padding = 2),\n","            nn.BatchNorm1d(512),\n","            nn.Tanh(),\n","            nn.Dropout(),\n","            nn.Conv1d(512, 512, 5, padding = 2),\n","            nn.BatchNorm1d(512),\n","            nn.Tanh(),\n","            nn.Dropout(),\n","            nn.Conv1d(512, 512, 5, padding = 2),\n","            nn.BatchNorm1d(512),\n","            nn.Tanh(),\n","            nn.Dropout(),\n","            nn.Conv1d(512, 512, 5, padding = 2),\n","            nn.BatchNorm1d(512),\n","            nn.Tanh(),\n","            nn.Dropout(),\n","            nn.Conv1d(512, NR_MELS, 5, padding = 2),\n","            nn.BatchNorm1d(NR_MELS),\n","            nn.Dropout()\n","        )\n","\n","\n","    def forward(self, context_vector, prev_frame, h_x):\n","        '''\n","        Args:\n","            context_vector: the attention context vector for current step (BATCH_SIZE, EMBEDDING_SIZE)\n","            prev_frame: previous step output (frame), using teacher forcing for training (NR_MELS, BATCH_SIZE)\n","            h_x: tuple containing last step's hidden and cell states\n","                 h_n shape: (num_layers = 2, BATCH_SIZE, hidden_size = 1024)\n","                 c_n shape: (num_layers = 2, BATCH_SIZE, hidden_size = 1024)\n","        '''\n","        prev_frame = prev_frame.transpose(1, 0)\n","        #assert prev_frame.shape == (BATCH_SIZE, 256), prev_frame.shape\n","        x = torch.concat([context_vector, prev_frame], dim = 1)\n","        #assert x.shape == (BATCH_SIZE, EMBEDDING_SIZE + 256), x.shape\n","        x = x.unsqueeze(1) # LSTM input shape: (batch_size, seq_len = 1, input_len)\n","        _, (h_n, c_n) = self.lstm(x, h_x)\n","        h_n = self.drop(h_n)\n","        #assert h_n.shape == (2, BATCH_SIZE, 1024), h_n.shape\n","        x = torch.concat([context_vector, h_n[1]], dim = 1)\n","        #assert x.shape == (BATCH_SIZE, EMBEDDING_SIZE + h_n.shape[2]), x.shape\n","        x_out = self.linear(x)\n","        x_stop = self.stop_linear(x)\n","        return (h_n, c_n), x_out, x_stop"]},{"cell_type":"markdown","metadata":{"id":"Zw3Puro7EUIB"},"source":["Decoder theo kiểu của Nvidia (chèn thông tin attention trên vào cả 2 layer tầng đàu và tầng sau GRU/LSTM)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y9f_NPI0Ezi1"},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, melspec_frame_shape):\n","        super().__init__()\n","        self.pre_net = nn.Sequential(\n","            nn.Linear(melspec_frame_shape, 256),\n","            nn.ReLU(),\n","            nn.Dropout(),\n","            nn.Linear(256, 256),\n","            nn.ReLU(),\n","            nn.Dropout()\n","        )\n","\n","        self.lstm_l1 = nn.LSTM(EMBEDDING_SIZE + 256, 1024, 1, batch_first = True)\n","        self.att_nn = None # the attention network\n","        self.lstm_l2 = nn.LSTM(EMBEDDING_SIZE + 1024, 1024, 1, batch_first = True)\n","\n","        self.linear = nn.Linear(EMBEDDING_SIZE + 1024, 80)\n","        self.stop_linear = nn.Sequential(\n","            nn.Linear(EMBEDDING_SIZE + 1024, 1),\n","        )\n","        self.post_net = nn.Sequential(\n","            nn.Conv1d(NR_MELS, 512, 5, padding = 2),\n","            nn.BatchNorm1d(512),\n","            nn.Tanh(),\n","            nn.Dropout(),\n","            nn.Conv1d(512, 512, 5, padding = 2),\n","            nn.BatchNorm1d(512),\n","            nn.Tanh(),\n","            nn.Dropout(),\n","            nn.Conv1d(512, 512, 5, padding = 2),\n","            nn.BatchNorm1d(512),\n","            nn.Tanh(),\n","            nn.Dropout(),\n","            nn.Conv1d(512, 512, 5, padding = 2),\n","            nn.BatchNorm1d(512),\n","            nn.Tanh(),\n","            nn.Dropout(),\n","            nn.Conv1d(512, NR_MELS, 5, padding = 2),\n","            nn.BatchNorm1d(NR_MELS),\n","            nn.Dropout()\n","        )\n","\n","\n","    def get_mask_from_lengths(self, lengths):\n","        max_len = torch.max(lengths).item()\n","        ids = torch.arange(0, max_len, out=torch.cuda.LongTensor(max_len))\n","        mask = (ids < lengths.unsqueeze(1)).bool()\n","        return mask\n","\n","\n","    def init_states(self, enc_out, trans_lens, batch_size, seq_len, train_mode = True):\n","        self.h_n1 = torch.zeros((1, batch_size, 1024), requires_grad = train_mode).to(DEVICE) # (nr_layers, BATCH_SIZE, hidden_size)\n","        self.c_n1 = torch.zeros((1, batch_size, 1024), requires_grad = train_mode).to(DEVICE) # (nr_layers, BATCH_SIZE, hidden_size)\n","        self.h_n2 = torch.zeros((1, batch_size, 1024), requires_grad = train_mode).to(DEVICE) # (nr_layers, BATCH_SIZE, hidden_size)\n","        self.c_n2 = torch.zeros((1, batch_size, 1024), requires_grad = train_mode).to(DEVICE) # (nr_layers, BATCH_SIZE, hidden_size)\n","\n","        self.prev_step_att = torch.zeros((batch_size, seq_len), requires_grad = train_mode).to(DEVICE)\n","        self.prev_steps_att = torch.zeros((batch_size, seq_len), requires_grad = train_mode).to(DEVICE)\n","\n","        self.context_vec = torch.zeros((batch_size, EMBEDDING_SIZE), requires_grad = train_mode).to(DEVICE)\n","\n","        self.enc_out = enc_out\n","        self.att_nn.align_nn.proj1 = self.att_nn.align_nn.ll_enc_hidden(enc_out)\n","        if trans_lens != None:\n","            self.att_nn.mask = ~self.get_mask_from_lengths(trans_lens)\n","\n","    def forward(self, prev_frame):\n","        '''\n","        Args:\n","            context_vector: the attention context vector for current step (BATCH_SIZE, EMBEDDING_SIZE)\n","            prev_frame: previous step output (frame), using teacher forcing for training (NR_MELS, BATCH_SIZE)\n","            h_x: tuple containing last step's hidden and cell states\n","                 h_n shape: (num_layers = 2, BATCH_SIZE, hidden_size = 1024)\n","                 c_n shape: (num_layers = 2, BATCH_SIZE, hidden_size = 1024)\n","        '''\n","        prev_frame = prev_frame.transpose(1, 0)\n","        #assert prev_frame.shape == (BATCH_SIZE, 256), prev_frame.shape\n","\n","        x = torch.concat([self.context_vec, prev_frame], dim = -1)\n","        #assert x.shape == (BATCH_SIZE, EMBEDDING_SIZE + 256), x.shape\n","        x = x.unsqueeze(1) # LSTM input shape: (batch_size, seq_len = 1, input_len)\n","        _, (self.h_n1, self.c_n1) = self.lstm_l1(x, (self.h_n1, self.c_n1))\n","        self.h_n1 = nn.functional.dropout(self.h_n1, 0.1, True)\n","\n","        att = torch.cat((self.prev_step_att.unsqueeze(1), self.prev_steps_att.unsqueeze(1)), dim = 1)\n","        self.prev_step_att, self.context_vec = self.att_nn(self.enc_out, self.h_n1.squeeze(0).unsqueeze(1), att)\n","        self.prev_steps_att += self.prev_step_att\n","\n","        x = torch.concat([self.h_n1.squeeze(0), self.context_vec], dim = -1).unsqueeze(1)\n","        _, (self.h_n2, self.c_n2) = self.lstm_l2(x, (self.h_n2, self.c_n2))\n","        self.h_n2 = nn.functional.dropout(self.h_n2, 0.1, True)\n","\n","        #assert h_n.shape == (2, BATCH_SIZE, 1024), h_n.shape\n","        x = torch.concat([self.context_vec, self.h_n2.squeeze(0)], dim = 1)\n","        #assert x.shape == (BATCH_SIZE, EMBEDDING_SIZE + h_n.shape[2]), x.shape\n","        x_out = self.linear(x)\n","        x_stop = self.stop_linear(x)\n","        return  x_out, x_stop"]},{"cell_type":"markdown","metadata":{"id":"A0BxSa1TE7lz"},"source":["mô hình tacotron2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GIoX8yM3E3PC"},"outputs":[],"source":["class Tacotron2(nn.Module):\n","    def __init__(self, enc_bi_lstm_size = 256, dec_lstm_size = 1024, train_mode = True):\n","        super().__init__()\n","\n","        self.train_mode = True\n","        self.prev_train_loss = float('inf')\n","        self.prev_valid_loss = float('inf')\n","\n","        self.enc = Encoder(VOCAB_SIZE, EMBEDDING_SIZE)\n","        self.dec = Decoder(NR_MELS)\n","        self.dec.att_nn = Attention(enc_bi_lstm_size * 2, dec_lstm_size)\n","\n","\n","        self.optimizer = torch.optim.Adam(self.parameters(), lr = LR, eps = EPS, weight_decay = WEIGHT_DECAY)\n","\n","\n","    def init_states(self, batch_size, seq_len):\n","        self.frame = torch.zeros((batch_size, NR_MELS), requires_grad = self.train_mode).to(DEVICE)\n","        self.h_n = torch.zeros((2, batch_size, 1024), requires_grad = self.train_mode).to(DEVICE) # (nr_layers, BATCH_SIZE, hidden_size)\n","        self.c_n = torch.zeros((2, batch_size, 1024), requires_grad = self.train_mode).to(DEVICE) # (nr_layers, BATCH_SIZE, hidden_size)\n","\n","        self.prev_step_att = torch.zeros((batch_size, seq_len), requires_grad = self.train_mode).to(DEVICE)\n","        self.prev_steps_att = torch.zeros((batch_size, seq_len), requires_grad = self.train_mode).to(DEVICE)\n","\n","\n","    def mask_outputs(self, melspecs, res_melspecs, stop_tokens, mel_lens):\n","        mel_mask = torch.zeros(melspecs.shape, dtype = torch.bool).to(DEVICE)\n","        stop_mask = torch.zeros(stop_tokens.shape, dtype = torch.bool).to(DEVICE)\n","        for i, len in enumerate(mel_lens):\n","            mel_mask[i, :, len:] = True\n","            stop_mask[i, len:] = True\n","\n","        melspecs.data.masked_fill_(mel_mask, 0.0)\n","        res_melspecs.data.masked_fill_(mel_mask, 0.0)\n","        stop_tokens.data.masked_fill_(stop_mask, 1e3)\n","\n","        return melspecs, res_melspecs, stop_tokens\n","\n","    def _forward(self, transcript, trans_lens, melspec, stop_tokens, mel_lens):\n","        #assert melspec.shape == (BATCH_SIZE, NR_MELS, torch.max(mel_lens)), melspec.shape\n","\n","        loss = 0.0\n","        x_outs, x_stops = [], []\n","\n","        enc_out = self.enc(transcript)\n","        self.init_states(transcript.shape[0], transcript.shape[-1])\n","        self.dec.init_states(enc_out, trans_lens, transcript.shape[0], transcript.shape[-1], self.train_mode)\n","        #self.att_nn.align_nn.proj1 = self.att_nn.align_nn.ll_enc_hidden(enc_out)\n","        #self.att_nn.mask = ~self.get_mask_from_lengths(trans_lens)\n","        melspec = self.dec.pre_net(melspec.permute(0, 2, 1))\n","        self.frame = self.dec.pre_net(self.frame).transpose(1, 0) # initial frame\n","        melspec = melspec.permute(1, 2, 0) # permute in order to traverse the frames\n","        #assert melspec.shape == (torch.max(mel_lens), 256, BATCH_SIZE), melspec.shape\n","        for i, crt_frame in enumerate(melspec):\n","            #self.prev_steps_att = self.prev_steps_att + self.prev_step_att\n","            #att = torch.cat((self.prev_step_att.unsqueeze(1), self.prev_steps_att.unsqueeze(1)), dim = 1)\n","            #self.prev_step_att, context_vec = self.att_nn(enc_out, self.h_n[1].unsqueeze(1), att)\n","            #assert context_vec.shape == (BATCH_SIZE, EMBEDDING_SIZE), context_vec.shape\n","            #(self.h_n, self.c_n), x_out, x_stop = self.dec(context_vec,\n","            #                                               self.frame,\n","            #                                               (self.h_n, self.c_n))\n","            x_out, x_stop = self.dec(self.frame)\n","            x_outs.append(x_out)\n","            x_stops.append(x_stop)\n","\n","            self.frame = crt_frame\n","\n","        x_out = torch.stack(x_outs, dim = 2)\n","        out = x_out + self.dec.post_net(x_out)\n","        x_stops = torch.cat(x_stops, dim = 1)\n","        return x_out, out, x_stops\n","\n","    def _inference(self, transcript):\n","        self.eval()\n","        with torch.no_grad():\n","            enc_out = self.enc(transcript)\n","            self.init_states(transcript.shape[0], transcript.shape[-1])\n","            self.dec.init_states(enc_out, None, transcript.shape[0], transcript.shape[-1])\n","            x_stop = torch.FloatTensor([-1.0])\n","            crt_steps = 0\n","            melspec = []\n","            frame = torch.zeros((transcript.shape[0], NR_MELS)).to(DEVICE)\n","            #self.att_nn.align_nn.proj1 = self.att_nn.align_nn.ll_enc_hidden(enc_out)\n","            while torch.sigmoid(x_stop).item() < THRESHOLD and crt_steps < MAX_DEC_STEPS:\n","                #self.prev_steps_att = self.prev_steps_att + self.prev_step_att\n","                #att = torch.cat((self.prev_step_att.unsqueeze(1), self.prev_steps_att.unsqueeze(1)), dim = 1)\n","                #self.prev_step_att, context_vec = self.att_nn(enc_out, self.h_n[1].unsqueeze(1), att)\n","                #assert context_vec.shape == (BATCH_SIZE, EMBEDDING_SIZE), context_vec.shape\n","                frame = self.dec.pre_net(frame).transpose(1, 0)\n","                x_out, x_stop = self.dec(frame)\n","                melspec.append(x_out)\n","                frame = x_out\n","                crt_steps += 1\n","            melspec = torch.stack(melspec, dim = 2)\n","            melspec = melspec + self.dec.post_net(melspec)\n","        return melspec.cpu().numpy()\n","\n","    def forward(self, transcript, trans_lens = None, melspec = None, stop_tokens = None, mel_lens = None):\n","        if self.train_mode:\n","            return self._forward(transcript, trans_lens, melspec, stop_tokens, mel_lens)\n","        else:\n","            return self._inference(transcript)\n","\n","    def criterion(self, y_pred, y):\n","        # taken from: https://github.com/NVIDIA/tacotron2\n","        x_out, out, x_stop = y_pred[0], y_pred[1], y_pred[2]\n","        melspec, stop_tokens = y\n","        loss = nn.MSELoss()(x_out, melspec) + \\\n","               nn.MSELoss()(out, melspec) + \\\n","               nn.BCEWithLogitsLoss()(x_stop, stop_tokens)\n","        return loss\n","\n","    def get_mask_from_lengths(self, lengths):\n","        # taken from: https://github.com/NVIDIA/tacotron2\n","        max_len = torch.max(lengths).item()\n","        ids = torch.arange(0, max_len, out=torch.cuda.LongTensor(max_len))\n","        mask = (ids < lengths.unsqueeze(1)).bool()\n","        return mask\n","\n","    def parse_outputs(self, melspecs, res_melspecs, stop_tokens, mel_lens):\n","        # taken from: https://github.com/NVIDIA/tacotron2\n","        if mel_lens is not None:\n","            mask = ~self.get_mask_from_lengths(mel_lens)\n","            mask = mask.expand(NR_MELS, mask.size(0), mask.size(1))\n","            mask = mask.permute(1, 0, 2)\n","\n","            melspecs.data.masked_fill_(mask, 0.0)\n","            res_melspecs.data.masked_fill_(mask, 0.0)\n","            stop_tokens.data.masked_fill_(mask[:, 0, :], 1e3)  #1e3 # gate energies\n","        return (melspecs, res_melspecs, stop_tokens)\n","\n","    def fit(self, train_loader, valid_loader):\n","        #writer = SummaryWriter('runs/taco2')\n","        train_losses, valid_losses = [], []\n","        self.train_mode = True\n","        self.train()\n","        for e in range(EPOCHS):\n","            total_loss = 0\n","            for (trans, melspec, stop_tokens, mel_lens, trans_lens) in train_loader:\n","                trans = trans.to(DEVICE)\n","                melspec = melspec.to(DEVICE)\n","                stop_tokens = stop_tokens.to(DEVICE)\n","                mel_lens = mel_lens.to(DEVICE)\n","                trans_lens = trans_lens.to(DEVICE)\n","\n","                out = self(trans, trans_lens, melspec, stop_tokens)\n","                out = self.parse_outputs(out[0], out[1], out[2], mel_lens)\n","                #assert x_out.shape == out.shape == melspec.shape\n","                loss = self.criterion(out, (melspec, stop_tokens))\n","                total_loss += loss.item()\n","\n","                self.zero_grad(set_to_none = True)\n","                loss.backward()\n","                torch.nn.utils.clip_grad_norm_(self.parameters(), 1.0)\n","                self.optimizer.step()\n","            total_loss /= len(train_loader)\n","            train_losses.append(total_loss)\n","            print(\"#{} TRAIN LOSS: {:.4f}\".format(e, total_loss), end = \"\\n\")\n","            #val_loss = self.validate(valid_loader)\n","            #valid_losses.append(val_loss)\n","            #if val_loss < self.prev_val_loss: # use this if training on the entire dataset\n","            #if total_loss < self.prev_train_loss:\n","            #print(\"Saving model...\")\n","            self.save(total_loss, valid_loss = None)\n","            #self.prev_val_loss = val_loss\n","            #self.prev_train_loss = total_loss\n","            plt.plot(train_losses)\n","            #plt.plot(valid_losses)\n","            #plt.savefig('train_loss.png')\n","\n","    def validate(self, loader):\n","        self.eval()\n","        with torch.no_grad():\n","            total_loss = 0.0\n","            for (trans, melspec, stop_tokens, mel_lens) in loader:\n","                trans = trans.to(DEVICE)\n","                melspec = melspec.to(DEVICE)\n","                stop_tokens = stop_tokens.to(DEVICE)\n","                out = self(trans, melspec, stop_tokens, mel_lens)\n","                loss = self.criterion(out, (melspec, stop_tokens))\n","                total_loss += loss.item()\n","            total_loss /= len(loader)\n","        self.train()\n","        print(\"EVAL LOSS: {:.4f}\".format(total_loss))\n","        return total_loss\n","\n","    def save(self, train_loss, valid_loss = None):\n","        torch.save({\n","            'train_loss': train_loss,\n","            'valid_loss': valid_loss,\n","            'model_state_dict': self.state_dict(),\n","            'optimizer_state_dict': self.optimizer.state_dict(),\n","            }, CHKP_PATH)\n","\n","    def load(self):\n","        checkpoint = torch.load(CHKP_PATH, map_location=torch.device(DEVICE))\n","        self.prev_train_loss = checkpoint['train_loss']\n","        self.prev_valid_loss = checkpoint['valid_loss']\n","        print(\"Current checkpoint train loss: {:.4f}\".format(checkpoint['train_loss']))\n","        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","        self.load_state_dict(checkpoint['model_state_dict'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MC78IoVxE94Z"},"outputs":[],"source":["ds = LJSpeech(\"/content/drive/MyDrive/project1/LJSpeech-1.1/LJSpeech/ljspeech_gpu.hkl\")\n","\n","\n","print(\"Dataset size:\", len(ds))\n","print(\"Transcript shape:\", ds[0][0].shape)\n","print(\"Melspectrogram shape:\", ds[0][1].shape)\n","print(\"Stop token shape:\", ds[0][2].shape)\n","\n","plt.figure(figsize = (8, 6), dpi = 100)\n","p = plt.imshow(ds[0][1][:, :].transpose(1, 0))\n","\n","train_ds, valid_ds, test_ds = torch.utils.data.random_split(ds, [12000, 95, 500], generator=torch.Generator().manual_seed(42))\n","\n","train_loader = DataLoader(train_ds, batch_size = BATCH_SIZE, shuffle = True, collate_fn = collate_pad)\n","valid_loader = DataLoader(valid_ds, batch_size = BATCH_SIZE, shuffle = True, collate_fn = collate_pad)\n","\n","# sample size: ((BATCH_SIZE, NR_WORDS), (BATCH_SIZE, NR_MELS, NR_FRAMES), (BATCH_SIZE, NR_FRAMES), (BATCH_SIZE, NR_FRAMES))\n","sample = next(iter(train_loader))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FXc7i6GrFBeC"},"outputs":[],"source":["taco = Tacotron2().to(DEVICE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rDA5zSjnFDjq"},"outputs":[],"source":["taco.load()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vXf0NTYiFEaz"},"outputs":[],"source":["taco.fit(train_loader, valid_loader)"]},{"cell_type":"markdown","source":["**Tinh chỉnh audio bằng HiFi-Gan**"],"metadata":{"id":"0MS1bI9sVMt8"}},{"cell_type":"code","source":["import IPython.display as ipd"],"metadata":{"id":"Hc2vgCXpVZdq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ipd.Audio('LJSpeech-1.1/wavs/LJ001-0001.wav')"],"metadata":{"id":"_-sb-sMQVbBY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# torch.Size([1, 80, 831])\n","test_loader = DataLoader(train_ds, batch_size = 1, shuffle = True, collate_fn = collate_pad)\n","sample = next(iter(test_loader))\n","sample = next(iter(test_loader))\n","transcript = sample[0].to(DEVICE)\n","orig_melspec = sample[1]\n","taco.eval()\n","taco.train_mode = False\n","melspec = taco(transcript) # the input is the transcript\n","print(orig_melspec.shape)\n","print(melspec.shape)\n","np.save(\"test_mel_files/original.npy\", orig_melspec[0].numpy())\n","np.save(\"test_mel_files/taco2_output.npy\", melspec[0])"],"metadata":{"id":"OfbgU7whVe8A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python inference_e2e.py --checkpoint_file generator_v3"],"metadata":{"id":"cJtqq5_YVhYy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ipd.Audio('generated_files_from_mel/original_generated_e2e.wav')"],"metadata":{"id":"L_KhdKHeViA6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ipd.Audio('generated_files_from_mel/taco2_output_generated_e2e.wav')"],"metadata":{"id":"coUT5u57VkGD"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyNLauylPITOyMzzqI/MLiyO"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}